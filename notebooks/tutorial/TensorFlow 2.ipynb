{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:07.485286Z",
     "start_time": "2020-01-31T01:59:07.480940Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:09.143971Z",
     "start_time": "2020-01-31T01:59:07.746412Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:09.147931Z",
     "start_time": "2020-01-31T01:59:09.145434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief summary of major changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- API cleanup. Removes redundant APIs, makes APIs more consistent.\n",
    "- Eager execution. Decorate a Python function using `tf.function()` to mark it for JIT compilation.\n",
    "- No more \"globals\". If you lose track of a `tf.Variable`, it gets garbage collected.\n",
    "\n",
    "See https://www.tensorflow.org/guide/effective_tf2 for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T06:26:10.302192Z",
     "start_time": "2020-01-14T06:26:10.295684Z"
    }
   },
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use NumPy array or `tf.data` API for data pipelining. Generally, for larger dataset, you want your data as a `tf.data.Dataset` object. A Dataset object can be **created** from data in memory or disk, and can be **transformed** to another Dataset.\n",
    "\n",
    "See https://www.tensorflow.org/guide/data for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:11.134993Z",
     "start_time": "2020-01-31T01:59:10.750976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from `tf.keras.datasets`.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # These are NumPy arrays.\n",
    "\n",
    "# Standardize data.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:12.211888Z",
     "start_time": "2020-01-31T01:59:11.695840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset, then shuffle and batch them.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture APIs (High-level to low-level):\n",
    "- Sequential model. Data goes through a sequence of layers.\n",
    "- Functional API. More flexible than Sequential model.\n",
    "- Layer subclassing. Subclass `tf.keras.layers.Layer` to create custom layer (custom computation blocks).\n",
    "- Model subclassing. Subclass `tf.keras.Model`. Like layer subclassing, but allow you to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:14.497685Z",
     "start_time": "2020-01-31T01:59:13.840855Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "], name='mnist_sequential')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:15.322996Z",
     "start_time": "2020-01-31T01:59:15.317748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:16.299926Z",
     "start_time": "2020-01-31T01:59:16.296092Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train from NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T01:59:44.664195Z",
     "start_time": "2020-01-31T01:59:18.126786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.2962 - accuracy: 0.9146 - val_loss: 0.1458 - val_accuracy: 0.9562\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.1428 - accuracy: 0.9574 - val_loss: 0.1015 - val_accuracy: 0.9706\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.1079 - accuracy: 0.9669 - val_loss: 0.0846 - val_accuracy: 0.9723\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0877 - accuracy: 0.9732 - val_loss: 0.0798 - val_accuracy: 0.9761\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0748 - accuracy: 0.9758 - val_loss: 0.0720 - val_accuracy: 0.9760\n",
      "Time taken : 26.533419609069824 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train from `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:00:29.390782Z",
     "start_time": "2020-01-31T01:59:57.151326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0652 - accuracy: 0.9790 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0584 - accuracy: 0.9812 - val_loss: 0.0751 - val_accuracy: 0.9772\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0539 - accuracy: 0.9818 - val_loss: 0.0753 - val_accuracy: 0.9783\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0486 - accuracy: 0.9839 - val_loss: 0.0708 - val_accuracy: 0.9791\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0441 - accuracy: 0.9852 - val_loss: 0.0719 - val_accuracy: 0.9782\n",
      "Time taken : 32.23463726043701 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:00:32.848262Z",
     "start_time": "2020-01-31T02:00:32.177644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07191426173503192, 0.9782]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:00:34.911819Z",
     "start_time": "2020-01-31T02:00:34.433194Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:00:41.717446Z",
     "start_time": "2020-01-31T02:00:41.645679Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:00:42.921842Z",
     "start_time": "2020-01-31T02:00:42.916509Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_functional\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:01:19.464790Z",
     "start_time": "2020-01-31T02:00:45.285367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.2891 - accuracy: 0.9158 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1515 - accuracy: 0.9558 - val_loss: 0.1238 - val_accuracy: 0.9660\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1243 - accuracy: 0.9644 - val_loss: 0.1076 - val_accuracy: 0.9699\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1100 - accuracy: 0.9697 - val_loss: 0.0997 - val_accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0985 - accuracy: 0.9733 - val_loss: 0.1082 - val_accuracy: 0.9732\n",
      "Time taken : 34.17536807060242 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-sequential example : ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:01:51.277799Z",
     "start_time": "2020-01-31T02:01:51.149134Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "block_1_output = tf.keras.layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='resnet_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:01:51.763327Z",
     "start_time": "2020-01-31T02:01:51.752297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_functional\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 26, 26, 32)   320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 24, 24, 64)   18496       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 64)     36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8, 8, 64)     0           conv2d_5[0][0]                   \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           40970       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 133,642\n",
      "Trainable params: 133,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:02:50.105930Z",
     "start_time": "2020-01-31T02:01:52.918911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1305 - accuracy: 0.9593 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0477 - accuracy: 0.9856 - val_loss: 0.0324 - val_accuracy: 0.9892\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0381 - accuracy: 0.9882 - val_loss: 0.0317 - val_accuracy: 0.9886\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0338 - accuracy: 0.9903 - val_loss: 0.0254 - val_accuracy: 0.9915\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0298 - accuracy: 0.9919 - val_loss: 0.0256 - val_accuracy: 0.9913\n",
      "Time taken : 57.18211579322815 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom \"layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:02:56.356742Z",
     "start_time": "2020-01-31T02:02:56.347498Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x += inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:02:57.343171Z",
     "start_time": "2020-01-31T02:02:57.167060Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = ResidualBlock()(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='custom_layer_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:03:21.603661Z",
     "start_time": "2020-01-31T02:03:21.595665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_layer_functional\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc (None, 8, 8, 64)          73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 133,642\n",
      "Trainable params: 133,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now the model looks \"sequential\".\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:03:24.953991Z",
     "start_time": "2020-01-31T02:03:24.935519Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(3)\n",
    "        self.residual = ResidualBlock()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.residual(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = CustomModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T02:03:30.345287Z",
     "start_time": "2020-01-31T02:03:26.915421Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer custom_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.2511 - accuracy: 0.5792 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5094 - accuracy: 0.8635 - val_loss: 0.4025 - val_accuracy: 0.8760\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 0.3530 - accuracy: 0.8938 - val_loss: 0.3467 - val_accuracy: 0.8927\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2526 - accuracy: 0.9177 - val_loss: 0.2837 - val_accuracy: 0.9115\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2049 - accuracy: 0.9323 - val_loss: 0.2283 - val_accuracy: 0.9354\n",
      "Time taken : 3.4268994331359863 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset.take(30), epochs=5, validation_data=test_dataset.take(30))\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training APIs (High-level to low-level):\n",
    "- Built-in training loops. (`model.compile(...)`, then `model.fit(...)`).\n",
    "- Writing training loops from scratch with `tf.GradientTape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T06:23:39.817509Z",
     "start_time": "2020-01-16T06:23:39.021687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from `tf.keras.datasets`.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # These are NumPy arrays.\n",
    "\n",
    "# Standardize data.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Create dataset, then shuffle and batch them.\n",
    "BUFFER = 1000\n",
    "BATCH_SIZE = 300\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T06:23:39.826146Z",
     "start_time": "2020-01-16T06:23:39.819234Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x += inputs\n",
    "        return x\n",
    "    \n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(3)\n",
    "        self.residual = ResidualBlock()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.residual(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to train and test one batch. Note that we're decorating the functions with `tf.function` to mark them for JIT compilations. We also create a function to perform the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:12:49.512280Z",
     "start_time": "2020-01-16T11:12:49.482235Z"
    },
    "code_folding": [
     21
    ]
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_on_batch(x, y):\n",
    "    \"\"\"Train one batch of (x, y)\"\"\"\n",
    "    # Compute loss while recording the gradient.\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss(y, y_pred)\n",
    "        \n",
    "    # Get gradient of weights w.r.t. loss.\n",
    "    grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Using optimizer, apply gradients to trainable weights.\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "        \n",
    "# @tf.function\n",
    "def test_on_batch(x, y):\n",
    "    \"\"\"Test one batch of (x, y)\"\"\"\n",
    "    # Compute loss.\n",
    "    y_pred = model(x, training=False)\n",
    "    loss_value = loss(y, y_pred)\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "    \n",
    "def train(train_data, epochs, validation_data=None):\n",
    "    \"\"\"Perform training loop.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        message = []\n",
    "        loss_metric_message = []\n",
    "        message.append(f'Epoch {epoch}/{epochs}')\n",
    "        \n",
    "        # Iterate through training dataset and\n",
    "        # train model on each batch.\n",
    "        for i, (x, y) in enumerate(train_data):\n",
    "            s = time.time()\n",
    "            train_on_batch(x, y)\n",
    "            print(time.time()-s)\n",
    "            \n",
    "            \n",
    "        # Obtain metric values after trained on all batches.\n",
    "        train_metric_values = {metric.name: metric.result().numpy() for metric in metrics}\n",
    "        train_loss_values = {'loss': loss_metric.result().numpy()}\n",
    "        # Reset metric states at the end of each epoch.\n",
    "        for metric in metrics:\n",
    "            metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        \n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_loss_values.items()]))\n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_metric_values.items()]))\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            # Iterate through validation dataset and\n",
    "            # train model on each batch.\n",
    "            for i, (x, y) in enumerate(validation_data):\n",
    "                test_on_batch(x, y)\n",
    "\n",
    "            # Obtain metric values after trained on all batches.\n",
    "            val_metric_values = {f'val_{metric.name}': metric.result().numpy() for metric in metrics}\n",
    "            val_loss_values = {'val_loss': loss_metric.result().numpy()}\n",
    "            # Reset metric states at the end of each epoch.\n",
    "            for metric in metrics:\n",
    "                metric.reset_states()\n",
    "            loss_metric.reset_states()\n",
    "                \n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_loss_values.items()]))\n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_metric_values.items()]))\n",
    "        \n",
    "        message.append(f'{(time.time() - start):.2f} sec')\n",
    "        message += loss_metric_message\n",
    "        message = ' - '.join(message)\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:12:50.606234Z",
     "start_time": "2020-01-16T11:12:50.578240Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_metric = tf.keras.metrics.Mean('loss')\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:13:22.628593Z",
     "start_time": "2020-01-16T11:12:53.643472Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(train_dataset.take(30), epochs=5, validation_data=test_dataset.take(30))\n",
    "train(train_dataset, epochs=5, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:Layer custom_model_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
    "\n",
    "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
    "\n",
    "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
    "\n",
    "Epoch 0/5 - 9.96 sec - loss: 0.1264 - sparse_categorical_accuracy: 0.9621 - val_loss: 0.0426 - val_sparse_categorical_accuracy: 0.9855\n",
    "Epoch 1/5 - 9.03 sec - loss: 0.0457 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9898\n",
    "Epoch 2/5 - 9.09 sec - loss: 0.0372 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.0259 - val_sparse_categorical_accuracy: 0.9911\n",
    "Epoch 3/5 - 9.06 sec - loss: 0.0327 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0263 - val_sparse_categorical_accuracy: 0.9908\n",
    "Epoch 4/5 - 9.12 sec - loss: 0.0290 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0239 - val_sparse_categorical_accuracy: 0.9923\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test - eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T03:25:05.141864Z",
     "start_time": "2020-01-16T03:25:05.108539Z"
    },
    "code_folding": [
     0,
     19,
     32
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_on_batch(x, y):\n",
    "    \"\"\"Train one batch of (x, y)\"\"\"\n",
    "    # Compute loss while recording the gradient.\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss(y, y_pred)\n",
    "        \n",
    "    # Get gradient of weights w.r.t. loss.\n",
    "    grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Using optimizer, apply gradients to trainable weights.\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "        \n",
    "def test_on_batch(x, y):\n",
    "    \"\"\"Test one batch of (x, y)\"\"\"\n",
    "    # Compute loss.\n",
    "    y_pred = model(x, training=False)\n",
    "    loss_value = loss(y, y_pred)\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "    \n",
    "def train(train_data, epochs, validation_data=None):\n",
    "    \"\"\"Perform training loop.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        message = []\n",
    "        loss_metric_message = []\n",
    "        message.append(f'Epoch {epoch}/{epochs}')\n",
    "        \n",
    "        # Iterate through training dataset and\n",
    "        # train model on each batch.\n",
    "        for i, (x, y) in enumerate(train_data):\n",
    "            train_on_batch(x, y)\n",
    "            \n",
    "        # Obtain metric values after trained on all batches.\n",
    "        train_metric_values = {metric.name: metric.result().numpy() for metric in metrics}\n",
    "        train_loss_values = {'loss': loss_metric.result().numpy()}\n",
    "        # Reset metric states at the end of each epoch.\n",
    "        for metric in metrics:\n",
    "            metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        \n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_loss_values.items()]))\n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_metric_values.items()]))\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            # Iterate through validation dataset and\n",
    "            # train model on each batch.\n",
    "            for i, (x, y) in enumerate(validation_data):\n",
    "                test_on_batch(x, y)\n",
    "\n",
    "            # Obtain metric values after trained on all batches.\n",
    "            val_metric_values = {f'val_{metric.name}': metric.result().numpy() for metric in metrics}\n",
    "            val_loss_values = {'val_loss': loss_metric.result().numpy()}\n",
    "            # Reset metric states at the end of each epoch.\n",
    "            for metric in metrics:\n",
    "                metric.reset_states()\n",
    "            loss_metric.reset_states()\n",
    "                \n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_loss_values.items()]))\n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_metric_values.items()]))\n",
    "        \n",
    "        message.append(f'{(time.time() - start):.2f} sec')\n",
    "        message += loss_metric_message\n",
    "        message = ' - '.join(message)\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T03:25:14.125813Z",
     "start_time": "2020-01-16T03:25:14.095038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_metric = tf.keras.metrics.Mean('loss')\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T03:28:41.361012Z",
     "start_time": "2020-01-16T03:25:16.828423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train(train_dataset.take(30), epochs=5, validation_data=test_dataset.take(30))\n",
    "train(train_dataset, epochs=5, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "WARNING:tensorflow:Layer custom_model_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
    "\n",
    "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
    "\n",
    "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
    "\n",
    "Epoch 0/5 - 41.06 sec - loss: 0.1298 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.0408 - val_sparse_categorical_accuracy: 0.9854\n",
    "Epoch 1/5 - 40.79 sec - loss: 0.0468 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0433 - val_sparse_categorical_accuracy: 0.9873\n",
    "Epoch 2/5 - 41.14 sec - loss: 0.0384 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9892\n",
    "Epoch 3/5 - 40.74 sec - loss: 0.0320 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0332 - val_sparse_categorical_accuracy: 0.9896\n",
    "Epoch 4/5 - 40.80 sec - loss: 0.0303 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0370 - val_sparse_categorical_accuracy: 0.9897\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - v1 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T06:07:25.217286Z",
     "start_time": "2020-01-16T06:07:25.207512Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train_ohe = np.eye(10)[y_train]\n",
    "y_test_ohe = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T06:08:05.236616Z",
     "start_time": "2020-01-16T06:07:56.413313Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_tf1():\n",
    "    \n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "    \n",
    "    epochs = 5\n",
    "    BATCH_SIZE=1000\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_ohe)).shuffle(1000).batch(BATCH_SIZE)\n",
    "    va_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test_ohe)).batch(BATCH_SIZE)\n",
    "\n",
    "    iterator = tf.data.Iterator.from_structure(\n",
    "        tr_dataset.output_types,\n",
    "        tr_dataset.output_shapes)\n",
    "\n",
    "    tr_initializer = iterator.make_initializer(tr_dataset)\n",
    "    va_initializer = iterator.make_initializer(va_dataset)\n",
    "\n",
    "    training = tf.placeholder_with_default(input=False, shape=(), name='training')\n",
    "\n",
    "    x, y = iterator.get_next()\n",
    "    x = tf.layers.conv2d(x, 32, 3, activation='relu')\n",
    "    x = tf.layers.conv2d(x, 64, 3, activation='relu')\n",
    "    x1 = tf.layers.max_pooling2d(x, 3, 3)\n",
    "\n",
    "    x = tf.layers.conv2d(x1, 64, 3, activation='relu', padding='same')\n",
    "    x = tf.layers.conv2d(x, 64, 3, activation='relu', padding='same')\n",
    "    x = tf.add(x, x1)\n",
    "\n",
    "    x = tf.layers.flatten(x)\n",
    "    x = tf.layers.dropout(x, 0.5, training=training)\n",
    "    logits = tf.layers.dense(x, 10)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    classes = tf.argmax(probs, axis=1)\n",
    "    true_classes = tf.argmax(y, axis=1)\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(y, logits)\n",
    "    acc = tf.metrics.accuracy(true_classes, classes)\n",
    "#     opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    opt = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        print(f'Init time: {time.time() - start}')\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "    #         s = time.time()\n",
    "            sess.run(tr_initializer)\n",
    "    #         e = time.time()\n",
    "    #         print(f'tr init - {e-s}')\n",
    "\n",
    "    #         s = time.time()\n",
    "            ls = []\n",
    "            accs = []\n",
    "            while True:\n",
    "                try:\n",
    "                    _, l, a = sess.run([opt, loss, acc], feed_dict={training:True})\n",
    "                    ls.append(np.mean(l))\n",
    "                    accs.append(np.mean(a))\n",
    "                except Exception as e:\n",
    "                    break\n",
    "            tl = np.mean(ls)\n",
    "            ta = np.mean(accs)\n",
    "    #         e = time.time()\n",
    "    #         print(f'tr loop - {e-s}')\n",
    "\n",
    "    #         s = time.time()\n",
    "            sess.run(va_initializer)\n",
    "    #         e = time.time()\n",
    "    #         print(f'va init - {e-s}')\n",
    "\n",
    "    #         s = time.time()\n",
    "            ls = []\n",
    "            accs = []\n",
    "            while True:\n",
    "                try:\n",
    "                    l, a = sess.run([loss, acc], feed_dict={training:False})\n",
    "                    ls.append(np.mean(l))\n",
    "                    accs.append(np.mean(a))\n",
    "                except Exception:\n",
    "                    break\n",
    "            vl = np.mean(ls)\n",
    "            va = np.mean(accs)\n",
    "    #         e = time.time()\n",
    "    #         print(f'va loop - {e-s}')\n",
    "\n",
    "            print(f'Epoch {epoch}/{epochs} - {(time.time()-start):.2f} sec - loss: {tl:.4f} - accuracy: {ta:.4f} - val_loss: {vl:.4f} - val_accuracy: {va:.4f}')\n",
    "    \n",
    "test_tf1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
