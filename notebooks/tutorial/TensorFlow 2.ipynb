{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:34:47.433126Z",
     "start_time": "2020-01-31T03:34:47.427483Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:34:49.007184Z",
     "start_time": "2020-01-31T03:34:47.666061Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:34:49.010860Z",
     "start_time": "2020-01-31T03:34:49.008471Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A brief summary of major changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- API cleanup. Removes redundant APIs, makes APIs more consistent.\n",
    "- Eager execution. Decorate a Python function using `tf.function()` to mark it for JIT compilation.\n",
    "- No more \"globals\". If you lose track of a `tf.Variable`, it gets garbage collected.\n",
    "\n",
    "See https://www.tensorflow.org/guide/effective_tf2 for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T06:26:10.302192Z",
     "start_time": "2020-01-14T06:26:10.295684Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can use NumPy array or `tf.data` API for data pipelining. Generally, for larger dataset, you want your data as a `tf.data.Dataset` object. A Dataset object can be **created** from data in memory or disk, and can be **transformed** to another Dataset.\n",
    "\n",
    "See https://www.tensorflow.org/guide/data for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:24:41.076859Z",
     "start_time": "2020-01-31T03:24:40.698967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from `tf.keras.datasets`.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # These are NumPy arrays.\n",
    "\n",
    "# Standardize data.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:24:41.476665Z",
     "start_time": "2020-01-31T03:24:41.078260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create dataset, then shuffle and batch them.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Model architecture APIs (High-level to low-level):\n",
    "- Sequential model. Data goes through a sequence of layers.\n",
    "- Functional API. More flexible than Sequential model.\n",
    "- Layer subclassing. Subclass `tf.keras.layers.Layer` to create custom layer (custom computation blocks).\n",
    "- Model subclassing. Subclass `tf.keras.Model`. Like layer subclassing, but allow you to use `.fit()`, `.evaluate()`, and `.predict()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:24:43.106333Z",
     "start_time": "2020-01-31T03:24:42.418993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "], name='mnist_sequential')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:24:43.850017Z",
     "start_time": "2020-01-31T03:24:43.841646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:24:45.504841Z",
     "start_time": "2020-01-31T03:24:45.500353Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train from NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:25:16.585975Z",
     "start_time": "2020-01-31T03:24:46.270419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train from `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:16:30.507878Z",
     "start_time": "2020-01-31T03:15:53.340694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:16:31.314517Z",
     "start_time": "2020-01-31T03:16:30.510233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:16:31.852377Z",
     "start_time": "2020-01-31T03:16:31.316686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:25:22.310479Z",
     "start_time": "2020-01-31T03:25:22.221975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:25:22.754135Z",
     "start_time": "2020-01-31T03:25:22.743595Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:25:26.948467Z",
     "start_time": "2020-01-31T03:25:23.691818Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Non-sequential example : ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:27:51.138822Z",
     "start_time": "2020-01-31T03:27:51.012272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "block_1_output = tf.keras.layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(block_1_output)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.add([x, block_1_output])\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='resnet_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:27:51.574492Z",
     "start_time": "2020-01-31T03:27:51.560465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:28:12.468125Z",
     "start_time": "2020-01-31T03:27:52.327961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Layer subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a custom \"layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.374493Z",
     "start_time": "2020-01-31T03:15:22.385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x += inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.375133Z",
     "start_time": "2020-01-31T03:15:22.386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = ResidualBlock()(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='custom_layer_functional')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.375799Z",
     "start_time": "2020-01-31T03:15:22.387Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now the model looks \"sequential\".\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.376515Z",
     "start_time": "2020-01-31T03:15:22.390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(3)\n",
    "        self.residual = ResidualBlock()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.residual(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = CustomModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.377183Z",
     "start_time": "2020-01-31T03:15:22.391Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "print(f'Time taken : {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training APIs (High-level to low-level):\n",
    "- Built-in training loops. (`model.compile(...)`, then `model.fit(...)`).\n",
    "- Writing training loops from scratch with `tf.GradientTape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:34:52.628608Z",
     "start_time": "2020-01-31T03:34:51.801954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from `tf.keras.datasets`.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # These are NumPy arrays.\n",
    "\n",
    "# Standardize data.\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add a channel dimension.\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Create dataset, then shuffle and batch them.\n",
    "BUFFER = 1000\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:35:01.403323Z",
     "start_time": "2020-01-31T03:35:01.296109Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x += inputs\n",
    "        return x\n",
    "    \n",
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(3)\n",
    "        self.residual = ResidualBlock()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.residual(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:36:18.487332Z",
     "start_time": "2020-01-31T03:36:18.462994Z"
    }
   },
   "outputs": [],
   "source": [
    "# TensorBoard writers\n",
    "train_log_dir = 'logs/basic/train'\n",
    "val_log_dir = 'logs/basic/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_metric = tf.keras.metrics.Mean('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to train and test one batch. Note that we're decorating the functions with `tf.function` to mark them for JIT compilations. We also create a function to perform the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:45:04.312650Z",
     "start_time": "2020-01-31T03:45:04.279331Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    \"\"\"Train one batch of (x, y)\"\"\"\n",
    "    # Compute loss while recording the gradient.\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss(y, y_pred)\n",
    "        \n",
    "    # Get gradient of weights w.r.t. loss.\n",
    "    grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Using optimizer, apply gradients to trainable weights.\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "        \n",
    "@tf.function\n",
    "def test_on_batch(x, y):\n",
    "    \"\"\"Test one batch of (x, y)\"\"\"\n",
    "    # Compute loss.\n",
    "    y_pred = model(x, training=False)\n",
    "    loss_value = loss(y, y_pred)\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "    \n",
    "def train(train_data, epochs, validation_data=None):\n",
    "    \"\"\"Perform training loop.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        message = []\n",
    "        loss_metric_message = []\n",
    "        message.append(f'Epoch {epoch}/{epochs}')\n",
    "        \n",
    "        # Iterate through training dataset and\n",
    "        # train model on each batch.\n",
    "        for i, (x, y) in enumerate(train_data):\n",
    "            train_on_batch(x, y)\n",
    "                    \n",
    "        # Obtain metric values, then write to TensorBoard\n",
    "        train_metric_values = {}\n",
    "        with train_summary_writer.as_default():\n",
    "            train_loss_values = {'loss': loss_metric.result().numpy()}\n",
    "            tf.summary.scalar('loss', loss_metric.result(), step=epoch)\n",
    "            for metric in metrics:\n",
    "                tf.summary.scalar(metric.name, metric.result(), step=epoch)\n",
    "                train_metric_values[metric.name] = metric.result().numpy()\n",
    "                \n",
    "        # Reset metric states at the end of each epoch.\n",
    "        for metric in metrics:\n",
    "            metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        \n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_loss_values.items()]))\n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_metric_values.items()]))\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            # Iterate through validation dataset and\n",
    "            # train model on each batch.\n",
    "            for i, (x, y) in enumerate(validation_data):\n",
    "                test_on_batch(x, y)\n",
    "\n",
    "            # Obtain metric values, then write to TensorBoard\n",
    "            val_metric_values = {}\n",
    "            with val_summary_writer.as_default():\n",
    "                val_loss_values = {'val_loss': loss_metric.result().numpy()}\n",
    "                tf.summary.scalar('loss', loss_metric.result(), step=epoch)\n",
    "                for metric in metrics:\n",
    "                    tf.summary.scalar(metric.name, metric.result(), step=epoch)\n",
    "                    val_metric_values[metric.name] = metric.result().numpy()\n",
    "\n",
    "            \n",
    "            # Reset metric states at the end of each epoch.\n",
    "            for metric in metrics:\n",
    "                metric.reset_states()\n",
    "            loss_metric.reset_states()\n",
    "                \n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_loss_values.items()]))\n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_metric_values.items()]))\n",
    "        \n",
    "        message.append(f'{(time.time() - start):.2f} sec')\n",
    "        message += loss_metric_message\n",
    "        message = ' - '.join(message)\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:45:47.262572Z",
     "start_time": "2020-01-31T03:45:04.752531Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(train_dataset.take(30), epochs=5, validation_data=test_dataset.take(30))\n",
    "train(train_dataset, epochs=5, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WARNING:tensorflow:Layer custom_model_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
    "\n",
    "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
    "\n",
    "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
    "\n",
    "Epoch 0/5 - 9.96 sec - loss: 0.1264 - sparse_categorical_accuracy: 0.9621 - val_loss: 0.0426 - val_sparse_categorical_accuracy: 0.9855\n",
    "Epoch 1/5 - 9.03 sec - loss: 0.0457 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9898\n",
    "Epoch 2/5 - 9.09 sec - loss: 0.0372 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.0259 - val_sparse_categorical_accuracy: 0.9911\n",
    "Epoch 3/5 - 9.06 sec - loss: 0.0327 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0263 - val_sparse_categorical_accuracy: 0.9908\n",
    "Epoch 4/5 - 9.12 sec - loss: 0.0290 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0239 - val_sparse_categorical_accuracy: 0.9923\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T03:17:10.381291Z",
     "start_time": "2020-01-31T03:15:22.400Z"
    },
    "code_folding": [
     26,
     46,
     59
    ]
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize base class.\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(3)\n",
    "        self.residual = ResidualBlock()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.pool(x)\n",
    "        # x = self.residual(x)\n",
    "        #if training:\n",
    "        #    x = self.dropout(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# @tf.function\n",
    "def train_on_batch(x, y):\n",
    "    \"\"\"Train one batch of (x, y)\"\"\"\n",
    "    # Compute loss while recording the gradient.\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss(y, y_pred)\n",
    "        \n",
    "    # Get gradient of weights w.r.t. loss.\n",
    "    grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Using optimizer, apply gradients to trainable weights.\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "        \n",
    "# @tf.function\n",
    "def test_on_batch(x, y):\n",
    "    \"\"\"Test one batch of (x, y)\"\"\"\n",
    "    # Compute loss.\n",
    "    y_pred = model(x, training=False)\n",
    "    loss_value = loss(y, y_pred)\n",
    "    \n",
    "    # Compute metrics. Metrics will accumulate values.\n",
    "    for metric in metrics:\n",
    "        metric(y, y_pred)\n",
    "        \n",
    "    # Record loss\n",
    "    loss_metric(loss_value)\n",
    "    \n",
    "def train(train_data, epochs, validation_data=None):\n",
    "    \"\"\"Perform training loop.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        message = []\n",
    "        loss_metric_message = []\n",
    "        message.append(f'Epoch {epoch}/{epochs}')\n",
    "        \n",
    "        # Iterate through training dataset and\n",
    "        # train model on each batch.\n",
    "        for i, (x, y) in enumerate(train_data):\n",
    "#             s = time.time()\n",
    "            train_on_batch(x, y)\n",
    "#             print(time.time()-s)\n",
    "            \n",
    "            \n",
    "        # Obtain metric values after trained on all batches.\n",
    "        train_metric_values = {metric.name: metric.result().numpy() for metric in metrics}\n",
    "        train_loss_values = {'loss': loss_metric.result().numpy()}\n",
    "        # Reset metric states at the end of each epoch.\n",
    "        for metric in metrics:\n",
    "            metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        \n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_loss_values.items()]))\n",
    "        loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in train_metric_values.items()]))\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            # Iterate through validation dataset and\n",
    "            # train model on each batch.\n",
    "            for i, (x, y) in enumerate(validation_data):\n",
    "                test_on_batch(x, y)\n",
    "\n",
    "            # Obtain metric values after trained on all batches.\n",
    "            val_metric_values = {f'val_{metric.name}': metric.result().numpy() for metric in metrics}\n",
    "            val_loss_values = {'val_loss': loss_metric.result().numpy()}\n",
    "            # Reset metric states at the end of each epoch.\n",
    "            for metric in metrics:\n",
    "                metric.reset_states()\n",
    "            loss_metric.reset_states()\n",
    "                \n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_loss_values.items()]))\n",
    "            loss_metric_message.append(' - '.join([f'{k}: {v:.4f}' for k,v in val_metric_values.items()]))\n",
    "        \n",
    "        message.append(f'{(time.time() - start):.2f} sec')\n",
    "        message += loss_metric_message\n",
    "        message = ' - '.join(message)\n",
    "        print(message)\n",
    "        \n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_metric = tf.keras.metrics.Mean('loss')\n",
    "\n",
    "model = CustomModel()\n",
    "\n",
    "# train(train_dataset.take(30), epochs=5, validation_data=test_dataset.take(30))\n",
    "train(train_dataset, epochs=5, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
